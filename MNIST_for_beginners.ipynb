{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "see [MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners)\n",
    "This example is a classifer<br>\n",
    " [jupyter cheat sheet](https://www.cheatography.com/weidadeyue/cheat-sheets/jupyter-notebook/)\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A future statement is a directive to the compiler that a particular \n",
    "module should be compiled using syntax or semantics that will be \n",
    "available in a specified future release of Python. \n",
    "\"\"\"\n",
    "# \n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys as sy\n",
    "\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your tensorflow and python versions. \n",
    "the Tensofrflow 1.1 API is available [here](https://www.tensorflow.org/versions/r1.1/api_docs/python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The python version is: 3.5.3 |Continuum Analytics, Inc.| (default, May 15 2017, 10:43:23) [MSC v.1900 64 bit (AMD64)]\n",
      "The TensorFlow version is: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The python version is:\", sy.version)\n",
    "print(\"The TensorFlow version is:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Begin by downloading the mnist data. Be patient, it could take a little time. \n",
    "Tensorflow includes utilities to download the data.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data_dir/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data_dir/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data_dir/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data_dir/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data_dir/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code above, you will have a new directory, *MNIST_data_dir*. It will contain 4 zipped files that containing the test data, training data, and validation data sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images in training set is 55000\n",
      "number of labels in training set is 55000\n",
      "number of images in test set is 10000\n",
      "number of labels in test set is 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of images in training set is\", len(mnist.train.images))\n",
    "print(\"number of labels in training set is\", len(mnist.train.labels))\n",
    "print(\"number of images in test set is\", len(mnist.test.images))\n",
    "print(\"number of labels in test set is\", len(mnist.test.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note one-hot encoding for labels [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "the first image is 7: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"note one-hot encoding for labels\",mnist.test.labels[0])\n",
    "print(\"the first image is 7:\",mnist.train.labels[0][7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a array of 784 pixels (28 x 28) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the pixels - each value is float between 0 and 1 \n",
    "# last image , 3rd pixel\n",
    "mnist.train.images[54999][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Softmax Regression</h4>\n",
    "For additional information on [Softmax](http://neuralnetworksanddeeplearning.com/chap3.html#softmax) \n",
    "<p>\n",
    "Begin by creating a placeholder for inputs. We create a tensorflow *placeholder* of size [None, 784]. None means any length - the placeholder can bb used for any number of images of 28*28 = 784 pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholder for inputs\n",
    "x = tf.placeholder(tf.float32,[None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create <code>tf.Variable</code> tensors for the weights and the biases. A **Variable** is a modifiable tensor that wil be be placed in the tensorflow operations graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the softmax operation** \n",
    "Matrix multiply each input pixel, x, by the corresponding weight W. Add the bias. Normalize for each class (label) using the softmax function. y will be vector of probabilities that sums to 1. The entry with the highest probability is the prediction for the input x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **cost** or **loss** of a model represent the error, or how far off the model is from the truth. A classifier attempts to minimize the loss. In this example, the loss is determined using *cross-entropy*. \n",
    "\n",
    "For additional reading on cross-entropy see: \n",
    "* [Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)\n",
    "* [What's an intutive way to think of cross entropy](https://www.quora.com/Whats-an-intuitive-way-to-think-of-cross-entropy)\n",
    "* [Khan academy](https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <msub>\n",
    "    <mi>H</mi>\n",
    "    <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "      <msup>\n",
    "        <mi>y</mi>\n",
    "        <mo>&#x2032;</mo>\n",
    "      </msup>\n",
    "    </mrow>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mi>y</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>=</mo>\n",
    "  <mo>&#x2212;<!-- − --></mo>\n",
    "  <munder>\n",
    "    <mo>&#x2211;<!-- ∑ --></mo>\n",
    "    <mi>i</mi>\n",
    "  </munder>\n",
    "  <msubsup>\n",
    "    <mi>y</mi>\n",
    "    <mi>i</mi>\n",
    "    <mo>&#x2032;</mo>\n",
    "  </msubsup>\n",
    "  <mi>log</mi>\n",
    "  <mo>&#x2061;<!-- ⁡ --></mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <msub>\n",
    "    <mi>y</mi>\n",
    "    <mi>i</mi>\n",
    "  </msub>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement cross-entropy in TensorFlow, add another <code>tf.placeholder</code>. The shape of the paceholder is [None, 10]. This placeholder show the true distribution for  any number of input images for each of the 10 labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [reduce_sum](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/reduce_sum) and [reduce_mean](https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/reduce_mean) for the cross_entropy computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that google recommends the use of <code> cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))</code> rather than the direct computation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " cross_entropy = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply **backpropagation** - [backward propagation of errors](http://colah.github.io/posts/2015-08-Backprop/). A common mechanism for backprogation is [**gradient descent**](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/). Gradient descent is used to minimize the error in the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5 is the learning rate. Learning rate describes the step size used in the gradient descent. The step size is the amount the variable is shifted in the direction that reduces the cost (error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the model\n",
    "Use <code> tf.InteractiveSession</code> to launch the tensorflow graph. Initialize the variables in the graph with <code>tf.global_variables_initializer</code>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, images from the training set. The use of small batches (stochastic gradient descent) is a technique that is used to minimize teh compute cost.  We use a batch_size of 100. Each element of batch_xs is an image. Each element of batch_ys is a one-hot vector of labels.  The xs and ys_batches are applied to the <code>tf.placeholder</code> elements defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step,feed_dict = {x:batch_xs, y_:batch_ys})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evalute the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy the model is 0.9225\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"The accuracy the model is\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
